{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "my_seed = 881003\n",
    "valid_ratio = 0.1\n",
    "eps = 1e-8\n",
    "min_clip_value = 1e-7\n",
    "max_clip_value = 1 - 1e-7\n",
    "X_train_fpath = './data/X_train'\n",
    "y_train_fpath = './data/Y_train'\n",
    "X_test_fpath = './data/X_test'\n",
    "y_test_fpath = './output_{}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy seed\n",
    "np.random.seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "X_train = pd.read_csv(X_train_fpath, index_col=['id'])\n",
    "X_test = pd.read_csv(X_test_fpath, index_col=['id'])\n",
    "y_train = pd.read_csv(y_train_fpath, index_col=['id']).to_numpy().astype('float64').flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = X_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X_train, X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = X.nunique()\n",
    "one = count[count == 1].index\n",
    "two = count[count == 2].index\n",
    "three_or_more = count[count >= 3].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.drop(columns=one)\n",
    "X[three_or_more] = (X[three_or_more] - X[three_or_more].mean()) / X[three_or_more].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X.iloc[:train_size, :], X.iloc[train_size:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy().astype('float64')\n",
    "X_test = X_test.to_numpy().astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some helper function as we can't use sklearn...\n",
    "# not needed\n",
    "def normalize(X, is_train=True, columns=None, X_mean=None, X_std=None):\n",
    "    if columns is None:\n",
    "        columns = np.arange(X.shape[1])\n",
    "    if is_train:\n",
    "        X_mean = np.mean(X[:, columns], axis=0).reshape(1, -1)\n",
    "        X_std = np.std(X[:, columns], axis=0).reshape(1, -1)\n",
    "    X[:, columns] = (X[:, columns] - X_mean) / (X_std + eps)\n",
    "    return X, X_mean, X_std # X_mean, X_std: (1 * feature_num)\n",
    "def train_test_split(X, y, test_size=0.25):\n",
    "    permu = np.arange(X.shape[0])#np.random.permutation(X.shape[0])\n",
    "    train_size = int(X.shape[0] * (1 - test_size) + 0.5)\n",
    "    return X[permu[:train_size]], X[permu[train_size:]], y[permu[:train_size]], y[permu[train_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48830, 508) (5426, 508) (48830,) (5426,)\n"
     ]
    }
   ],
   "source": [
    "# Normalize them and split into train and validate\n",
    "#X_train, X_mean, X_std = normalize(X_train)\n",
    "#X_test, _, _ = normalize(X_test, is_train=False, X_mean=X_mean, X_std=X_std)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_ratio)\n",
    "\n",
    "# Check shapes\n",
    "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54256, 508) (54256,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for training\n",
    "def shuffle(X, y):\n",
    "    permu = np.random.permutation(X.shape[0])\n",
    "    return (X[permu], y[permu])\n",
    "def sigmoid(z):\n",
    "    return np.clip(1 / (1.0 + np.exp(-z)), min_clip_value, max_clip_value)\n",
    "def accuracy(y_pred, y_true):\n",
    "    return 1.0 - np.mean(np.abs(y_pred - y_true))\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.dot(y_true, np.log(y_pred)) - np.dot((1 - y_true), np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    def f(self, X, w, b):\n",
    "        return sigmoid(X @ w + b)\n",
    "    def predict(self, X, w, b):\n",
    "        return np.round(self.f(X, w, b)).astype(np.int)\n",
    "    def predict_f(self, X):\n",
    "        return np.round(self.f(X, self.w, self.b)).astype(np.int)\n",
    "    def gradient(self, X, y, w, b):\n",
    "        y_pred = self.f(X, w, b)\n",
    "        y_diff = y - y_pred\n",
    "        w_grad = -np.sum(y_diff * X.T, axis=1)\n",
    "        b_grad = -np.sum(y_diff)\n",
    "        return (w_grad, b_grad)\n",
    "    def fit(self, X_train, y_train, X_valid=None, y_valid=None, epochs=10, batch_size=10, learning_rate=0.2, print_every=1):\n",
    "        train_loss, valid_loss = [], []\n",
    "        train_acc, valid_acc = [], []\n",
    "        w, b = np.zeros((X_train.shape[1],)), np.zeros((1,))\n",
    "        step, bst, bstf, bt = 1, 0, None, 0\n",
    "        mw, mb = np.zeros((X_train.shape[1],)), np.zeros((1,))\n",
    "        vw, vb = np.zeros((X_train.shape[1],)), np.zeros((1,))\n",
    "        b1, b2, eps = 0.9, 0.999, 1e-9\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "            for i in range(0, X_train.shape[0], batch_size):\n",
    "                X = X_train[i * batch_size:(i + 1) * batch_size]\n",
    "                y = y_train[i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "                w_grad, b_grad = self.gradient(X, y, w, b)\n",
    "            \n",
    "                #mw = mw * b1 + w_grad * (1 - b1)\n",
    "                #vw = vw * b2 + w_grad ** 2 * (1 - b2)\n",
    "                #mhat = mw / (1 - b1)\n",
    "                #vhat = vw / (1 - b2)              \n",
    "                #w -= learning_rate * mhat / (np.sqrt(vhat) + eps)\n",
    "                #\n",
    "                #mb = mb * b1 + b_grad * (1 - b1)\n",
    "                #vb = vb * b2 + b_grad ** 2 * (1 - b2)\n",
    "                #mhat = mb / (1 - b1)\n",
    "                #vhat = vb / (1 - b2)              \n",
    "                #b -= learning_rate * mhat / (np.sqrt(vhat) + eps)\n",
    "                \n",
    "                w -= learning_rate / np.sqrt(step) * w_grad\n",
    "                b -= learning_rate / np.sqrt(step) * b_grad\n",
    "                \n",
    "                step += 1\n",
    "        \n",
    "            y_pred = self.f(X_train, w, b)\n",
    "            train_loss.append(cross_entropy_loss(y_pred, y_train) / X_train.shape[0])\n",
    "            train_acc.append(accuracy(np.round(y_pred), y_train))\n",
    "            \n",
    "            if X_valid is not None:\n",
    "                y_pred = self.f(X_valid, w, b)\n",
    "                valid_loss.append(cross_entropy_loss(y_pred, y_valid) / X_valid.shape[0])\n",
    "                valid_acc.append(accuracy(np.round(y_pred), y_valid))\n",
    "            \n",
    "                if valid_acc[-1] > bst:\n",
    "                    bst = valid_acc[-1]\n",
    "                    bstf = (w, b)\n",
    "                    bt = epoch\n",
    "\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                if X_valid is not None:\n",
    "                    print(epoch, train_loss[-1], train_acc[-1], valid_loss[-1], valid_acc[-1])\n",
    "                else:\n",
    "                    print(epoch, train_loss[-1], train_acc[-1])\n",
    "        \n",
    "        if X_valid is not None:\n",
    "            self.w = bstf[0]\n",
    "            self.b = bstf[1]\n",
    "            print(bt)\n",
    "        else:\n",
    "            self.w = w\n",
    "            self.b = b\n",
    "        return train_loss, train_acc, valid_loss, valid_acc\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.2906037606558233 0.875773090313332 0.2986431729177455 0.8730187983781792\n",
      "1 0.28280621824039925 0.8778005324595536 0.2910976466490936 0.8741245853298931\n",
      "2 0.28147344066456575 0.8783944296539012 0.29148359037259375 0.876889052709178\n",
      "3 0.2797851759518624 0.8794593487610076 0.289593828772404 0.877441946185035\n",
      "4 0.2796578944504862 0.8804833094409175 0.29030162670727305 0.875783265757464\n",
      "5 0.2806168646906679 0.8787220970714724 0.2907748064517746 0.8730187983781792\n",
      "6 0.27701800981108887 0.8798484538193734 0.2868627043280211 0.8763361592333211\n",
      "7 0.2772403245433671 0.8801761212369446 0.2865012732319169 0.8763361592333211\n",
      "8 0.2772882565826631 0.8800327667417571 0.2882013018206433 0.8750460744563214\n",
      "9 0.2757492156325615 0.8809133729264796 0.2853691051625078 0.8783634353114633\n",
      "10 0.27542193949303057 0.8805652262953103 0.2853670198557576 0.875783265757464\n",
      "11 0.2747274795559469 0.8815277493344256 0.2858353179580257 0.8763361592333211\n",
      "12 0.2738727235905559 0.8817530206840057 0.2846793697244961 0.8781791374861777\n",
      "13 0.2737860961080067 0.8814867909072291 0.2844259738633947 0.877994839660892\n",
      "14 0.27362413574016586 0.8816506246160147 0.2842891808422355 0.8778105418356064\n",
      "15 0.2744226470572362 0.8811796027032562 0.28522079539529505 0.8785477331367489\n",
      "16 0.2736411593678679 0.8810157689944706 0.28417825401833974 0.8770733505344637\n",
      "17 0.27305229084050814 0.8822854802375588 0.2834585144927875 0.8778105418356064\n",
      "18 0.27473416727167466 0.8805242678681139 0.2859236853744149 0.8765204570586067\n",
      "19 0.2739424326534994 0.881568707761622 0.2842341212596953 0.8759675635827497\n",
      "20 0.2723631250903543 0.8823059594511571 0.2828209184594631 0.877441946185035\n",
      "21 0.2724695458120577 0.8829817734998976 0.2831910732157145 0.879100626612606\n",
      "22 0.2724202337718354 0.8818758959655949 0.28267696927833663 0.8785477331367489\n",
      "23 0.27316769328490786 0.8822035633831661 0.2832781504799078 0.875783265757464\n",
      "24 0.2745182845025483 0.8818963751791931 0.2840645264035293 0.8759675635827497\n",
      "25 0.2723681193543954 0.8823264386647552 0.28307853936079136 0.8792849244378916\n",
      "26 0.2723998742802157 0.8823673970919517 0.28236144849991474 0.877994839660892\n",
      "27 0.27364741944228127 0.8816301454024166 0.2835629360441352 0.877994839660892\n",
      "28 0.2720793946272913 0.8829817734998976 0.2823299919239624 0.8785477331367489\n",
      "29 0.27778760652372225 0.8800942043825517 0.28744831848449415 0.875783265757464\n",
      "30 0.27139024535689316 0.8828998566455049 0.28191617311615935 0.8798378179137486\n",
      "31 0.2716993895324482 0.8826541060823264 0.2826580229123425 0.8794692222631773\n",
      "32 0.2712875105618003 0.8828384190047103 0.2821403705333126 0.8789163287873203\n",
      "33 0.2714536145986506 0.8832480032766742 0.2824161810177454 0.8767047548838923\n",
      "34 0.2711660737586843 0.8825721892279337 0.28222023470962554 0.8794692222631773\n",
      "35 0.27094448946923316 0.8833708785582634 0.2818909023791539 0.879653520088463\n",
      "36 0.271077920475826 0.8833299201310669 0.281986443848979 0.8778105418356064\n",
      "37 0.2717489584460341 0.882817939791112 0.2827735214841522 0.8772576483597494\n",
      "38 0.2712050040828575 0.8829408150727012 0.28230409246348476 0.8781791374861777\n",
      "39 0.27127549752837404 0.8832070448494778 0.28277621431138106 0.8798378179137486\n",
      "40 0.2714300360058778 0.8828793774319066 0.2828634755086812 0.879653520088463\n",
      "41 0.2724065273821101 0.8821626049559697 0.28402675028737673 0.8759675635827497\n",
      "42 0.2728426811313925 0.8814458324800327 0.2844277418559631 0.8781791374861777\n",
      "43 0.27059826825608624 0.8830432111406922 0.2817179553662744 0.8798378179137486\n",
      "44 0.27120701274647174 0.8823878763055498 0.2823920134239389 0.8792849244378916\n",
      "45 0.2712900704914052 0.8833094409174688 0.2821496502133999 0.879100626612606\n",
      "46 0.2715578797267439 0.8831865656358796 0.2824139975381208 0.877994839660892\n",
      "47 0.2704162569294841 0.8835551914806471 0.2812177552587683 0.88020641356432\n",
      "48 0.2704135670917778 0.8835142330534508 0.28144525646314317 0.879653520088463\n",
      "49 0.27028014223906965 0.8834323161990579 0.2815054289387074 0.8794692222631773\n",
      "50 0.2702951463814417 0.8832684824902723 0.28143430953366433 0.8789163287873203\n",
      "51 0.2704161160112429 0.8827360229367193 0.2815737129598324 0.879653520088463\n",
      "52 0.27052311155373837 0.8835142330534508 0.2816238898462584 0.8772576483597494\n",
      "53 0.27051611334499037 0.882715543723121 0.2818795422864089 0.879653520088463\n",
      "54 0.2705041757993983 0.8829817734998976 0.2820784639242577 0.88020641356432\n",
      "55 0.2700847901433139 0.8835756706942454 0.28142570816502993 0.8803907113896056\n",
      "56 0.27041992253234876 0.8827360229367193 0.2820384691796674 0.8800221157390343\n",
      "57 0.2703937297253028 0.8841286094613967 0.2822330438806952 0.8783634353114633\n",
      "58 0.26995015011616547 0.8834937538398525 0.28153806027511313 0.8803907113896056\n",
      "59 0.2700228598521582 0.8832684824902723 0.2816615252936635 0.8803907113896056\n",
      "55\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc, valid_loss, valid_acc = model.fit(X_train, y_train, X_valid, y_valid, batch_size=10, epochs=60, print_every=1, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.3078077598611844 0.8700788852845769\n",
      "1 0.29147566323790997 0.8749078442937187\n",
      "2 0.2982283703622238 0.8721800353877912\n",
      "3 0.28826844006747837 0.8752211736950752\n",
      "4 0.281681877371279 0.8792207313476851\n",
      "5 0.2818674735411991 0.8792023002064288\n",
      "6 0.2798897268276171 0.8790917133588911\n",
      "7 0.2795960488187585 0.8793681804777351\n",
      "8 0.2784088810306598 0.8798842524329107\n",
      "9 0.27920856912813663 0.8786862282512533\n",
      "10 0.2772391559367418 0.8805846358006488\n",
      "11 0.27753210351264185 0.880603066941905\n",
      "12 0.27641930113092567 0.8804740489531112\n",
      "13 0.2766603153037415 0.8797920967266293\n",
      "14 0.275947272161964 0.8811007077558243\n",
      "15 0.2761207237759734 0.8820959893836626\n",
      "16 0.27563483790783294 0.8815983485697434\n",
      "17 0.27953806058869446 0.8801607195517547\n",
      "18 0.27494187671272097 0.8805477735181363\n",
      "19 0.27493392972011055 0.8814508994396933\n",
      "20 0.2744056983650432 0.8814140371571808\n",
      "21 0.27395711901185493 0.8810454143320554\n",
      "22 0.27398528371111097 0.8811744323208494\n",
      "23 0.27455585663926313 0.8818748156885874\n",
      "24 0.27363835452480434 0.8815430551459746\n",
      "25 0.2763524952073566 0.8816167797109997\n",
      "26 0.2733092889620916 0.8814508994396933\n",
      "27 0.27403082189572747 0.8807873783544677\n",
      "28 0.2740621854546673 0.8816536419935123\n",
      "29 0.27337962290778134 0.8806399292244176\n",
      "30 0.27312868781532823 0.8811375700383368\n",
      "31 0.2730822164091139 0.8814877617222059\n",
      "32 0.2729306296727607 0.8818563845473312\n",
      "33 0.27393011928052907 0.8806583603656738\n",
      "34 0.27288131498629953 0.8821328516661752\n",
      "35 0.27361713325222575 0.88052934237688\n",
      "36 0.2740838678281214 0.8806583603656738\n",
      "37 0.27322600790805257 0.8817457976997936\n",
      "38 0.2723380302045423 0.8812297257446181\n",
      "39 0.2727491384606891 0.882390887643763\n",
      "40 0.2736915107864629 0.8824830433500442\n",
      "41 0.2723125047075334 0.881985402536125\n",
      "42 0.2722158955779685 0.881708935417281\n",
      "43 0.2725681386708489 0.8826120613388381\n",
      "44 0.27179781484316395 0.8817826599823061\n",
      "45 0.2720130002783323 0.8822434385137128\n",
      "46 0.27250066013339413 0.8827042170451195\n",
      "47 0.2725592036643885 0.8827779416101444\n",
      "48 0.27299329504992315 0.8808611029194928\n",
      "49 0.2716989649952366 0.8818010911235624\n",
      "50 0.2718044329914141 0.8822250073724565\n",
      "51 0.27179391642564843 0.8817642288410499\n",
      "52 0.27224758417058903 0.8827963727514008\n",
      "53 0.2716365427164402 0.881837953406075\n",
      "54 0.27185655315126184 0.8827779416101444\n",
      "55 0.2715766632488524 0.8820591271011501\n",
      "56 0.2715638345761872 0.8818563845473312\n",
      "57 0.27140813971133815 0.8822250073724565\n",
      "58 0.2715834585050297 0.882188145089944\n",
      "59 0.27136627739861735 0.8824461810675317\n",
      "0.27136627739861735 0.8824461810675317\n"
     ]
    }
   ],
   "source": [
    "tl, ta, _, _ = model.fit(X_train, y_train, batch_size=10, epochs=60, print_every=1, learning_rate=0.5)\n",
    "print(tl[-1], ta[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = pd.DataFrame(model.predict_f(X_test).transpose())\n",
    "y_test_pred.columns = ['label']\n",
    "y_test_pred['id'] = range(0, X_test.shape[0])\n",
    "y_test_pred = y_test_pred.reindex(columns=['id', 'label'])\n",
    "y_test_pred.to_csv(y_test_fpath.format('logistic'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
