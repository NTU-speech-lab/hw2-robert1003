{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "my_seed = 880301\n",
    "valid_ratio = 0.1\n",
    "eps = 1e-8\n",
    "min_clip_value = 1e-7\n",
    "max_clip_value = 1 - 1e-7\n",
    "X_train_fpath = './data/X_train'\n",
    "y_train_fpath = './data/Y_train'\n",
    "X_test_fpath = './data/X_test'\n",
    "y_test_fpath = './output_{}.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set numpy seed\n",
    "np.random.seed(my_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read datasets\n",
    "X_train = pd.read_csv(X_train_fpath, index_col=['id']).to_numpy().astype('float64')\n",
    "X_test = pd.read_csv(X_test_fpath, index_col=['id']).to_numpy().astype('float64')\n",
    "y_train = pd.read_csv(y_train_fpath, index_col=['id']).to_numpy().astype('float64').flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define some helper function as we can't use sklearn...\n",
    "def normalize(X, is_train=True, columns=None, X_mean=None, X_std=None):\n",
    "    if columns is None:\n",
    "        columns = np.arange(X.shape[1])\n",
    "    if is_train:\n",
    "        X_mean = np.mean(X[:, columns], axis=0).reshape(1, -1)\n",
    "        X_std = np.std(X[:, columns], axis=0).reshape(1, -1)\n",
    "    X[:, columns] = (X[:, columns] - X_mean) / (X_std + eps)\n",
    "    return X, X_mean, X_std # X_mean, X_std: (1 * feature_num)\n",
    "def train_test_split(X, y, test_size=0.25):\n",
    "    permu = np.arange(X.shape[0])#np.random.permutation(X.shape[0])\n",
    "    train_size = int(X.shape[0] * (1 - test_size) + 0.5)\n",
    "    return X[permu[:train_size]], X[permu[train_size:]], y[permu[:train_size]], y[permu[train_size:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48830, 510) (5426, 510) (48830,) (5426,)\n"
     ]
    }
   ],
   "source": [
    "# Normalize them and split into train and validate\n",
    "X_train, X_mean, X_std = normalize(X_train)\n",
    "X_test, _, _ = normalize(X_test, is_train=False, X_mean=X_mean, X_std=X_std)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=valid_ratio)\n",
    "\n",
    "# Check shapes\n",
    "print(X_train.shape, X_valid.shape, y_train.shape, y_valid.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function for training\n",
    "def shuffle(X, y):\n",
    "    permu = np.random.permutation(X.shape[0])\n",
    "    return (X[permu], y[permu])\n",
    "def sigmoid(z):\n",
    "    return np.clip(1 / (1.0 + np.exp(-z)), min_clip_value, max_clip_value)\n",
    "def accuracy(y_pred, y_true):\n",
    "    return 1.0 - np.mean(np.abs(y_pred - y_true))\n",
    "def cross_entropy_loss(y_pred, y_true):\n",
    "    return -np.dot(y_true, np.log(y_pred)) - np.dot((1 - y_true), np.log(1 - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "    def f(self, X, w, b):\n",
    "        return sigmoid(X @ w + b)\n",
    "    def predict(self, X, w, b):\n",
    "        return np.round(self.f(X, w, b)).astype(np.int)\n",
    "    def predict_f(self, X):\n",
    "        return np.round(self.f(X, self.w, self.b)).astype(np.int)\n",
    "    def gradient(self, X, y, w, b):\n",
    "        y_pred = self.f(X, w, b)\n",
    "        y_diff = y - y_pred\n",
    "        w_grad = -np.sum(y_diff * X.T, axis=1)\n",
    "        b_grad = -np.sum(y_diff)\n",
    "        return (w_grad, b_grad)\n",
    "    def fit(self, X_train, y_train, X_valid, y_valid, epochs=10, batch_size=10, learning_rate=0.2, print_every=1):\n",
    "        train_loss, valid_loss = [], []\n",
    "        train_acc, valid_acc = [], []\n",
    "        w, b = np.zeros((X_train.shape[1],)), np.zeros((1,))\n",
    "        step, bst, bstf, bt = 1, 0, None, 0\n",
    "        mw, mb = np.zeros((X_train.shape[1],)), np.zeros((1,))\n",
    "        vw, vb = np.zeros((X_train.shape[1],)), np.zeros((1,))\n",
    "        b1, b2, eps = 0.9, 0.999, 1e-9\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            X_train, y_train = shuffle(X_train, y_train)\n",
    "            for i in range(0, X_train.shape[0], batch_size):\n",
    "                X = X_train[i * batch_size:(i + 1) * batch_size]\n",
    "                y = y_train[i * batch_size:(i + 1) * batch_size]\n",
    "        \n",
    "                w_grad, b_grad = self.gradient(X, y, w, b)\n",
    "            \n",
    "                #mw = mw * b1 + w_grad * (1 - b1)\n",
    "                #vw = vw * b2 + w_grad ** 2 * (1 - b2)\n",
    "                #mhat = mw / (1 - b1)\n",
    "                #vhat = vw / (1 - b2)              \n",
    "                #w -= learning_rate * mhat / (np.sqrt(vhat) + eps)\n",
    "                #\n",
    "                #mb = mb * b1 + b_grad * (1 - b1)\n",
    "                #vb = vb * b2 + b_grad ** 2 * (1 - b2)\n",
    "                #mhat = mb / (1 - b1)\n",
    "                #vhat = vb / (1 - b2)              \n",
    "                #b -= learning_rate * mhat / (np.sqrt(vhat) + eps)\n",
    "                \n",
    "                w -= learning_rate / np.sqrt(step) * w_grad\n",
    "                b -= learning_rate / np.sqrt(step) * b_grad\n",
    "                \n",
    "                step += 1\n",
    "        \n",
    "            y_pred = self.f(X_train, w, b)\n",
    "            train_loss.append(cross_entropy_loss(y_pred, y_train) / X_train.shape[0])\n",
    "            train_acc.append(accuracy(np.round(y_pred), y_train))\n",
    "            \n",
    "            y_pred = self.f(X_valid, w, b)\n",
    "            valid_loss.append(cross_entropy_loss(y_pred, y_valid) / X_valid.shape[0])\n",
    "            valid_acc.append(accuracy(np.round(y_pred), y_valid))\n",
    "            \n",
    "            \n",
    "            if valid_acc[-1] > bst:\n",
    "                bst = valid_acc[-1]\n",
    "                bstf = (w, b)\n",
    "                bt = epoch\n",
    "\n",
    "            if (epoch + 1) % print_every == 0:\n",
    "                print(epoch, train_loss[-1], train_acc[-1], valid_loss[-1], valid_acc[-1])\n",
    "        \n",
    "        self.w = bstf[0]\n",
    "        self.b = bstf[1]\n",
    "        print(bt)\n",
    "        return train_loss, train_acc, valid_loss, valid_acc\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert1003/milk/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.7331928906198477 0.8599836166291215 0.7823241920909041 0.8514559528197567\n",
      "1 0.5685348541613852 0.8685234486995699 0.6302942632377245 0.8593807593070402\n",
      "2 0.5138805449951698 0.8719844357976654 0.5741013844481375 0.8639882049391817\n",
      "3 0.47569254750922924 0.8705918492729879 0.5185038191588905 0.86490969406561\n",
      "4 0.453281029366943 0.8731107925455662 0.4900789016452888 0.866015481017324\n",
      "5 0.43514380477758535 0.8733565431087447 0.46596860240325666 0.8682270549207519\n",
      "6 0.4131427945721683 0.8744828998566455 0.44717803294307956 0.8708072244747512\n",
      "7 0.39748472102678767 0.8763260290804833 0.42604508219779524 0.8706229266494655\n",
      "8 0.3881986243818092 0.8762031537988941 0.41857741842812207 0.8715444157758938\n",
      "9 0.37385266391337413 0.877329510546795 0.4084374931303828 0.8724659049023221\n",
      "10 0.3689814631679963 0.8783534712267049 0.39898214506426494 0.8719130114264652\n",
      "11 0.36153854628551546 0.8763669875076797 0.39198043111945546 0.8706229266494655\n",
      "12 0.3559967558533965 0.8779643661683392 0.3879161951737909 0.8708072244747512\n",
      "13 0.34664569560112496 0.8785787425762851 0.3770105349034461 0.8724659049023221\n",
      "14 0.33830632606376315 0.8779848453819373 0.3713224864895526 0.8737559896793218\n",
      "15 0.331145698380794 0.8793569526930166 0.35795415639643785 0.8737559896793218\n",
      "16 0.3294005833797466 0.8793774319066148 0.3539262048183743 0.8763361592333211\n",
      "17 0.32412639801679727 0.8796436616833914 0.34606519103575906 0.8744931809804645\n",
      "18 0.3231411615210046 0.8796027032561949 0.34800111935193717 0.8761518614080354\n",
      "19 0.32049524127881507 0.8808314560720868 0.3475885387685673 0.8750460744563214\n",
      "20 0.31865935448480126 0.8796641408969895 0.3436032243563156 0.8755989679321784\n",
      "21 0.3155717587495264 0.8800327667417571 0.3432407313862656 0.8724659049023221\n",
      "22 0.31218407777327345 0.8808314560720868 0.3395179578125201 0.8772576483597494\n",
      "23 0.30734813475002554 0.8816506246160147 0.33704010669299633 0.8750460744563214\n",
      "24 0.3038288779377013 0.8814458324800327 0.33126625046698643 0.8755989679321784\n",
      "25 0.3017988062999331 0.8821421257423715 0.33325926333430217 0.8761518614080354\n",
      "26 0.3031759952346453 0.8813434364120418 0.33738331156897106 0.8720973092517508\n",
      "27 0.3002101406964375 0.8814048740528364 0.3333712187408629 0.8761518614080354\n",
      "28 0.2998072303215556 0.8812000819168544 0.3343743474536872 0.8743088831551787\n",
      "29 0.30029155345024533 0.8818349375383985 0.33360941393323684 0.8755989679321784\n",
      "30 0.2972159982483407 0.8821830841695679 0.33075962100298406 0.8739402875046074\n",
      "31 0.29627347427001244 0.8805242678681139 0.32807727465464304 0.8737559896793218\n",
      "32 0.29528898812640125 0.881568707761622 0.3295160813205839 0.8704386288241799\n",
      "33 0.294799566304243 0.8827974605775138 0.327121576649453 0.8720973092517508\n",
      "34 0.2924556922638028 0.8833708785582634 0.3228907132062932 0.8755989679321784\n",
      "35 0.29279164472530694 0.8807290600040958 0.32592463443167113 0.8730187983781792\n",
      "36 0.2925235653518506 0.8812000819168544 0.3248148138773296 0.8750460744563214\n",
      "37 0.29053135025626187 0.88363710833504 0.3219879457412542 0.8741245853298931\n",
      "38 0.29005556508546115 0.8832070448494778 0.32374597231605884 0.8737559896793218\n",
      "39 0.2892496159542231 0.8830432111406922 0.3190990096233224 0.8733873940287504\n",
      "40 0.2894133913037522 0.8830636903542904 0.3195867134668182 0.8767047548838923\n",
      "41 0.2882277209046046 0.8828998566455049 0.32040842294943894 0.8735716918540362\n",
      "42 0.2881109304204422 0.883022731927094 0.3181354347007296 0.8739402875046074\n",
      "43 0.2882259259833967 0.8820192504607823 0.31981522838632415 0.8722816070770365\n",
      "44 0.28655228432375474 0.8824083555191481 0.31988015722950525 0.8750460744563214\n",
      "45 0.28550611195132 0.882715543723121 0.3165890516020034 0.8746774788057501\n",
      "46 0.28540562409292486 0.8834118369854598 0.3171366415919167 0.8739402875046074\n",
      "47 0.28790998057234124 0.8822445218103625 0.31896716959209964 0.8715444157758938\n",
      "48 0.28551501061019385 0.8826541060823264 0.31489621382850536 0.8750460744563214\n",
      "49 0.28472069242853887 0.8826950645095228 0.315952461680456 0.8728345005528935\n",
      "50 0.28466128828488796 0.8819373336063895 0.3150775851083734 0.8755989679321784\n",
      "51 0.2841783348858643 0.8831456072086832 0.30982652106121583 0.8776262440103206\n",
      "52 0.2846985920989439 0.8825312308007373 0.3117524087448829 0.8735716918540362\n",
      "53 0.28255179942459574 0.8828793774319066 0.3106478506939306 0.8759675635827497\n",
      "54 0.28329263919678127 0.882817939791112 0.3110387855766876 0.875230372281607\n",
      "55 0.2825534584640077 0.8840876510342003 0.30921963259882107 0.8781791374861777\n",
      "56 0.2842760437158645 0.8824083555191481 0.312319064842418 0.8728345005528935\n",
      "57 0.2812473290427449 0.8835756706942454 0.31228392170475494 0.8755989679321784\n",
      "58 0.2812569995083242 0.8832480032766742 0.30996062585346024 0.8755989679321784\n",
      "59 0.2816899339289781 0.8834732746262544 0.3132210010362556 0.8702543309988942\n",
      "60 0.28203526156060815 0.883227524063076 0.3121258080810387 0.8722816070770365\n",
      "61 0.2797937883306424 0.8841490886749949 0.30995970245270693 0.8726502027276078\n",
      "62 0.2820123376893527 0.8823673970919517 0.3132885378434752 0.8720973092517508\n",
      "63 0.2801716683386733 0.8833094409174688 0.3088154788529772 0.8735716918540362\n",
      "64 0.2803759544936659 0.8833094409174688 0.30894378769555814 0.876889052709178\n",
      "65 0.2799514631990593 0.8838623796846201 0.3114296175791208 0.8713601179506082\n",
      "66 0.27983223803555207 0.8839238173254147 0.30803052283728743 0.8730187983781792\n",
      "67 0.2798637365120729 0.8832684824902723 0.308463882573639 0.8732030962034648\n",
      "68 0.27915415386828824 0.8834118369854598 0.30919329765264775 0.8724659049023221\n",
      "69 0.2786084301589109 0.8845996313741552 0.3072920729991709 0.8763361592333211\n",
      "70 0.27873677137688974 0.8835142330534508 0.30677209229053964 0.8765204570586067\n",
      "71 0.27880688639833595 0.8840057341798075 0.3074212676948507 0.8770733505344637\n",
      "72 0.2787397571241465 0.883125127995085 0.31021106087850964 0.8754146701068928\n",
      "73 0.278797812026695 0.8837190251894327 0.3103965327085353 0.875230372281607\n",
      "74 0.2782808856966677 0.8842310055293877 0.3071730843674176 0.8783634353114633\n",
      "75 0.27890595766386733 0.8834937538398525 0.30912112896813315 0.8748617766310358\n",
      "76 0.2778944115318057 0.8833299201310669 0.3078732906760246 0.875783265757464\n",
      "77 0.2789435687043214 0.8831865656358796 0.3085973675480167 0.8770733505344637\n",
      "78 0.27822447871816824 0.8832070448494778 0.306632553423794 0.8761518614080354\n",
      "79 0.2770485211771046 0.8844357976653696 0.3071079689811163 0.8746774788057501\n",
      "80 0.2771132198681743 0.8844972353061642 0.3048490491879831 0.8761518614080354\n",
      "81 0.2771107884596771 0.8839238173254147 0.30519279053604015 0.8787320309620347\n",
      "82 0.2776221778110354 0.883227524063076 0.3064645008600379 0.8755989679321784\n",
      "83 0.2775500337900251 0.883739504403031 0.3042448815938959 0.8759675635827497\n",
      "84 0.27750244536072055 0.8827974605775138 0.3062978432529694 0.8763361592333211\n",
      "85 0.27645627234326203 0.8834732746262544 0.3063728403659206 0.8750460744563214\n",
      "86 0.27623061186402215 0.8842924431701823 0.30629838712029706 0.8715444157758938\n",
      "87 0.2759605439157673 0.8840057341798075 0.3041013067077104 0.876889052709178\n",
      "88 0.2767502038478867 0.8833094409174688 0.30510486231337275 0.8741245853298931\n",
      "89 0.2754811922771533 0.885091132500512 0.30322784238206635 0.8761518614080354\n",
      "90 0.2762640232983747 0.8844153184517715 0.302996900515079 0.877994839660892\n",
      "91 0.2769891960830206 0.884169567888593 0.30801912623350813 0.8733873940287504\n",
      "92 0.2759241457187017 0.884374360024575 0.3050120287365286 0.8739402875046074\n",
      "93 0.2756415528477468 0.8838214212574237 0.3046057309833107 0.8737559896793218\n",
      "94 0.27658863215557433 0.883739504403031 0.3035809231191443 0.8761518614080354\n",
      "95 0.27539267623011143 0.884681548228548 0.3014257468506163 0.876889052709178\n",
      "96 0.27549136390679785 0.8836575875486381 0.30330563683292167 0.8748617766310358\n",
      "97 0.27535537291437684 0.8844153184517715 0.3037401020483372 0.8744931809804645\n",
      "98 0.2751761312854766 0.8843538808109769 0.3025520408698512 0.8785477331367489\n",
      "99 0.2746108436162922 0.8848453819373336 0.30339348889790074 0.8746774788057501\n",
      "81\n"
     ]
    }
   ],
   "source": [
    "train_loss, train_acc, valid_loss, valid_acc = model.fit(X_train, y_train, X_valid, y_valid, epochs=100, batch_size=5, print_every=1, learning_rate=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/robert1003/milk/lib/python3.7/site-packages/ipykernel_launcher.py:6: RuntimeWarning: overflow encountered in exp\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "y_test_pred = pd.DataFrame(model.predict_f(X_test).transpose())\n",
    "y_test_pred.columns = ['label']\n",
    "y_test_pred['id'] = range(0, X_test.shape[0])\n",
    "y_test_pred = y_test_pred.reindex(columns=['id', 'label'])\n",
    "y_test_pred.to_csv(y_test_fpath.format('logistic'), index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
